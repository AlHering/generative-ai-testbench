{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f745113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca906779",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = os.path.abspath(\"\").replace(\"/src/langchain_basics\", \"\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc31b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain, FewShotPromptTemplate\n",
    "from src.configuration import configuration as cfg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cc387a9",
   "metadata": {},
   "source": [
    "(The following code is based off of https://github.com/samwit/langchain-tutorials/blob/main/)\n",
    "\n",
    "# Basic LLM setup and usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f5d3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /mnt/Workspaces/Workspaces/python/generative-ai-testbench/machine_learning_models/MODELS/eachadea_ggml-vicuna-7b-1.1/ggml-vicuna-7b-1.1-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 4 (mostly Q4_1, some F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  68.20 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(model_path=os.path.join(cfg.PATHS.TEXTGENERATION_MODEL_PATH,\n",
    "                            \"eachadea_ggml-vicuna-7b-1.1/ggml-vicuna-7b-1.1-q4_0.bin\"),\n",
    "               temperature=0.9,\n",
    "               max_tokens = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23f3e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To get to the other side and cool off!\n",
      "Why was the scarecrow so sad?\n",
      "Because he had no brain.\n"
     ]
    }
   ],
   "source": [
    "text = \"Why did the duck cross the road?\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d10adf0",
   "metadata": {},
   "source": [
    "# Promt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6a6e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_template = \"\"\"\n",
    "I want you to act as a naming consultant for new restaurants.\n",
    "Return a list of restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\n",
    "What are some good names for a restaurant that is {restaurant_desription}?\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"restaurant_desription\"],\n",
    "    template=restaurant_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d4cf35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI want you to act as a naming consultant for new restaurants.\\nReturn a list of restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\\nWhat are some good names for a restaurant that is a Greek place that serves fresh lamb souvlakis and other Greek food ?\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description = \"a Greek place that serves fresh lamb souvlakis and other Greek food \"\n",
    "description_02 = \"a burger place that is themed with baseball memorabilia\"\n",
    "description_03 = \"a cafe that has live hard rock music and memorabilia\"\n",
    "\n",
    "## to see what the prompt will be like\n",
    "prompt_template.format(restaurant_desription=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5cf445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rock N' Roll Cafe and Hard Rock Caf√© come to mind.\n"
     ]
    }
   ],
   "source": [
    "## querying the model with the prompt template\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(description_03))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95c890d0",
   "metadata": {},
   "source": [
    "# Using Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89cc3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create the list of few shot examples.\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d89187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we specify the template to format the examples we have provided.\n",
    "# We use the `PromptTemplate` class for this.\n",
    "example_formatter_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\\n\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c402dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we create the `FewShotPromptTemplate` object.\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # These are the examples we want to insert into the prompt.\n",
    "    examples=examples,\n",
    "    # This is how we want to format the examples when we insert them into the prompt.\n",
    "    example_prompt=example_prompt,\n",
    "    # The prefix is some text that goes before the examples in the prompt.\n",
    "    # Usually, this consists of intructions.\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    # The suffix is some text that goes after the examples in the prompt.\n",
    "    # Usually, this is where the user input will go\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    # The input variables are the variables that the overall prompt expects.\n",
    "    input_variables=[\"input\"],\n",
    "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
    "    example_separator=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dc81d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "\n",
      "\n",
      "Word: big\n",
      "Antonym:\n"
     ]
    }
   ],
   "source": [
    "# We can now generate a prompt using the `format` method.\n",
    "print(few_shot_prompt.format(input=\"big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd0da414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Small\n",
      "\n",
      "\n",
      "Word: beautiful\n",
      "Antonym: ugly\n",
      "\n",
      "Word: right\n",
      "Antonym: wrong\n",
      "\n",
      "Word: hot\n",
      "Antonym: cold\n",
      "\n",
      "Word: left\n",
      "Antonym: right\n",
      "\n",
      "Word: up\n",
      "Antonym: down\n",
      "\n",
      "Word: long\n",
      "Antonym: short\n",
      "\n",
      "Word: in\n",
      "Antonym: out\n",
      "\n",
      "Word: outside\n",
      "Antonym: inside\n",
      "\n",
      "Word: heavy\n",
      "Antonym: light\n",
      "\n",
      "Word: fast\n",
      "Antonym: slow\n",
      "\n",
      "Word: good\n",
      "Antonym: bad\n",
      "\n",
      "Word: upwards\n",
      "Antonym: downwards\n",
      "\n",
      "Word: forward\n",
      "Antonym: backward\n",
      "\n",
      "Word: outward\n",
      "Antonym: inward\n",
      "\n",
      "Word: central\n",
      "Antonym: peripheral\n",
      "\n",
      "Word: internal\n",
      "Antonym: external\n",
      "\n",
      "Word: digital\n",
      "Antonym: analog\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"Big\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51986205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
